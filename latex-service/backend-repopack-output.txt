This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-05T21:30:47.486Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
app/
  api/
    routes.py
  config.py
  latex_compiler.py
  logging_config.py
  main.py
  storage.py
scripts/
  debug.sh
  install_texlive.sh
.env.example
docker-compose.yml
Dockerfile
README.md
requirements.txt
structure.txt

================================================================
Repository Files
================================================================

================
File: app/api/routes.py
================
# app/api/routes.py
from fastapi import APIRouter, HTTPException, BackgroundTasks, UploadFile, File, Request, Query
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import List, Optional
import os
from PIL import Image
import io
from datetime import datetime
import traceback
import logging

from app.latex_compiler import LatexCompiler
from app.config import settings

router = APIRouter()
logger = logging.getLogger("latex-service")

class CompilationRequest(BaseModel):
    tex_content: str
    project_id: str
    output_filename: str

class ImageMetadata(BaseModel):
    file_path: str
    url: str
    file_type: str
    size: int
    created_at: str
    name: str
    dimensions: Optional[dict] = None

async def validate_image(file: UploadFile) -> None:
    """Validate image file before processing"""
    logger.debug(f"Validating image: {file.filename} ({file.content_type})")
    
    # Check file type
    if file.content_type not in settings.ALLOWED_IMAGE_TYPES:
        logger.warning(f"Invalid file type: {file.content_type}")
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type. Allowed types: {', '.join(settings.ALLOWED_IMAGE_TYPES)}"
        )
    
    # Check file size
    contents = await file.read()
    file_size = len(contents)
    await file.seek(0)  # Reset file pointer
    
    logger.debug(f"File size: {file_size / 1024 / 1024:.2f}MB")
    
    if file_size > settings.MAX_IMAGE_SIZE:
        logger.warning(f"File too large: {file_size / 1024 / 1024:.2f}MB")
        raise HTTPException(
            status_code=400,
            detail=f"File too large. Maximum size: {settings.MAX_IMAGE_SIZE / (1024 * 1024)}MB"
        )
    
    # Optionally validate and optimize image dimensions
    if settings.OPTIMIZE_IMAGES and file.content_type != 'application/pdf':
        try:
            img = Image.open(io.BytesIO(contents))
            width, height = img.size
            logger.debug(f"Image dimensions: {width}x{height}")
            
            if width > settings.MAX_IMAGE_DIMENSION or height > settings.MAX_IMAGE_DIMENSION:
                logger.warning(f"Image dimensions too large: {width}x{height}")
                raise HTTPException(
                    status_code=400,
                    detail=f"Image dimensions too large. Maximum dimension: {settings.MAX_IMAGE_DIMENSION}px"
                )
        except Exception as e:
            logger.error(f"Error processing image dimensions: {str(e)}")
            raise HTTPException(
                status_code=400,
                detail=f"Error processing image: {str(e)}"
            )
    
    logger.debug("Image validation successful")

@router.get("/health")
async def health_check(request: Request):
    """Health check endpoint"""
    logger.debug("Health check requested")
    storage_type = "local" if settings.IS_LOCAL else "supabase"
    return {
        "status": "healthy",
        "environment": settings.ENV,
        "storage": storage_type,
        "timestamp": datetime.now().isoformat()
    }

@router.post("/upload-image/{project_id}")
async def upload_image(
    request: Request,
    project_id: str,
    file: UploadFile = File(...),
    optimize: bool = Query(False, description="Whether to optimize the image before saving")
) -> ImageMetadata:
    """Upload an image file"""
    try:
        logger.info(f"Starting image upload for project {project_id}")
        logger.debug(f"File details - name: {file.filename}, content_type: {file.content_type}")
        
        # Validate image
        await validate_image(file)
        logger.debug("Image validation passed")
        
        storage_provider = request.app.state.storage_provider
        
        # Get original file extension
        _, ext = os.path.splitext(file.filename)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        new_filename = f"img_{timestamp}{ext}"
        
        logger.debug(f"Saving image with filename: {new_filename}")
        
        # Save image using storage provider
        file_path, url = await storage_provider.save_image(
            file,
            project_id,
            new_filename,
            optimize=optimize
        )
        
        logger.debug(f"Image saved successfully at {file_path}")
        
        # Get image dimensions if possible
        dimensions = None
        if file.content_type != 'application/pdf':
            contents = await file.read()
            await file.seek(0)
            try:
                with Image.open(io.BytesIO(contents)) as img:
                    dimensions = {"width": img.width, "height": img.height}
                logger.debug(f"Image dimensions: {dimensions}")
            except Exception as e:
                logger.warning(f"Could not get image dimensions: {e}")

        response = ImageMetadata(
            file_path=file_path,
            url=url,
            file_type=file.content_type,
            size=file.size if hasattr(file, 'size') else 0,
            created_at=datetime.now().isoformat(),
            name=new_filename,
            dimensions=dimensions
        )
        
        logger.info(f"Successfully uploaded image {new_filename} for project {project_id}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during image upload: {str(e)}\n{''.join(traceback.format_tb(e.__traceback__))}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/images/{project_id}")
async def list_images(
    request: Request,
    project_id: str,
    page: int = Query(1, ge=1),
    limit: int = Query(50, ge=1, le=100)
) -> dict:
    """List all images for a project"""
    logger.info(f"Listing images for project {project_id} (page {page}, limit {limit})")
    try:
        storage_provider = request.app.state.storage_provider
        images = await storage_provider.list_images(project_id, skip=(page-1)*limit, limit=limit)
        total = await storage_provider.count_images(project_id)
        
        logger.debug(f"Found {total} total images, returning {len(images)} for current page")
        
        return {
            "status": "success",
            "images": images,
            "pagination": {
                "page": page,
                "limit": limit,
                "total": total,
                "pages": (total + limit - 1) // limit
            }
        }
    except Exception as e:
        logger.error(f"Error listing images: {str(e)}\n{''.join(traceback.format_tb(e.__traceback__))}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/images/{project_id}/{image_path:path}")
async def delete_image(
    request: Request,
    project_id: str,
    image_path: str
) -> dict:
    """Delete an image"""
    logger.info(f"Deleting image {image_path} from project {project_id}")
    try:
        storage_provider = request.app.state.storage_provider
        success = await storage_provider.delete_image(project_id, image_path)
        
        if not success:
            logger.warning(f"Image not found: {image_path}")
            raise HTTPException(status_code=404, detail="Image not found")
            
        logger.info(f"Successfully deleted image {image_path}")
        return {
            "status": "success",
            "message": "Image deleted successfully"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting image: {str(e)}\n{''.join(traceback.format_tb(e.__traceback__))}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/compile")
async def compile_document(
    request: Request,
    compilation_request: CompilationRequest,
    background_tasks: BackgroundTasks
) -> dict:
    """Compile LaTeX document"""
    logger.info(f"Starting LaTeX compilation for project {compilation_request.project_id}")
    try:
        storage_provider = request.app.state.storage_provider
        compiler = LatexCompiler(storage_provider)
        
        logger.debug("Initializing compilation process")
        # Compile the document
        try:
            pdf_path = await compiler.compile(
                compilation_request.tex_content,
                compilation_request.project_id
            )
        except HTTPException as he:
            # Log the detailed error and return it to the client
            logger.error(f"LaTeX compilation failed:\n{he.detail.get('log', '')}")
            raise

        logger.debug(f"Compilation successful, saving PDF to storage: {compilation_request.output_filename}")
        # Save to storage
        file_path, url = await storage_provider.save_pdf(
            pdf_path,
            compilation_request.project_id,
            compilation_request.output_filename
        )
        
        logger.debug("Scheduling cleanup task")
        # Schedule cleanup in background
        background_tasks.add_task(compiler.cleanup_old_files)
        
        logger.info(f"Successfully compiled document for project {compilation_request.project_id}")
        return {
            "status": "success",
            "file_path": file_path,
            "url": url,
            "storage_type": "local" if settings.IS_LOCAL else "supabase",
            "compiled_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during compilation: {str(e)}\n{''.join(traceback.format_tb(e.__traceback__))}")
        raise HTTPException(
            status_code=500,
            detail={
                "message": "Internal server error during compilation",
                "error": str(e)
            }
        )

@router.get("/compile/status/{project_id}/{filename}")
async def get_compilation_status(
    request: Request,
    project_id: str,
    filename: str
) -> dict:
    """Get status of compiled PDF"""
    logger.info(f"Checking compilation status for {filename} in project {project_id}")
    try:
        storage_provider = request.app.state.storage_provider
        exists = await storage_provider.check_pdf_exists(project_id, filename)
        
        if not exists:
            logger.debug(f"PDF not found: {filename}")
            return {
                "status": "not_found",
                "exists": False
            }
            
        url = await storage_provider.get_pdf_url(project_id, filename)
        logger.debug(f"PDF found with URL: {url}")
        
        return {
            "status": "success",
            "exists": True,
            "url": url
        }
    except Exception as e:
        logger.error(f"Error checking compilation status: {str(e)}\n{''.join(traceback.format_tb(e.__traceback__))}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/config")
async def get_config() -> dict:
    """Get public configuration settings"""
    logger.debug("Retrieving public configuration settings")
    return {
        "max_image_size_mb": settings.MAX_IMAGE_SIZE / (1024 * 1024),
        "allowed_image_types": settings.ALLOWED_IMAGE_TYPES,
        "max_image_dimension": settings.MAX_IMAGE_DIMENSION,
        "storage_type": "local" if settings.IS_LOCAL else "supabase",
        "optimize_images": settings.OPTIMIZE_IMAGES
    }

================
File: app/config.py
================
# app/config.py
import os
from typing import List
from dotenv import load_dotenv

load_dotenv(override=True)

class Settings:
    # Environment settings
    ENV = os.getenv("ENV", "development")
    IS_LOCAL = ENV == "development"

    # Supabase settings
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    PDF_BUCKET_NAME = os.getenv("PDF_BUCKET_NAME", "pdfs")
    
    # Storage settings
    LOCAL_STORAGE_DIR = os.getenv("LOCAL_STORAGE_DIR", "storage")
    TEMP_DIR = os.getenv("TEMP_DIR", "/tmp/latex")
    
    # Compilation settings
    MAX_COMPILATION_TIME = int(os.getenv("MAX_COMPILATION_TIME", "300"))
    
    # Image settings
    MAX_IMAGE_SIZE = int(os.getenv("MAX_IMAGE_SIZE", "10")) * 1024 * 1024  # Default 10MB
    ALLOWED_IMAGE_TYPES: List[str] = [
        "image/jpeg",
        "image/png",
        "image/gif",
        "image/webp",
        "application/pdf"  # For vector graphics
    ]
    IMAGE_STORAGE_PATH = os.getenv("IMAGE_STORAGE_PATH", "images")  # Subfolder for images
    
    # Image processing settings
    OPTIMIZE_IMAGES = os.getenv("OPTIMIZE_IMAGES", "false").lower() == "true"
    MAX_IMAGE_DIMENSION = int(os.getenv("MAX_IMAGE_DIMENSION", "2000"))  # Max width/height
    
    @property
    def image_storage_dir(self) -> str:
        """Get the full path for image storage"""
        return os.path.join(self.LOCAL_STORAGE_DIR, self.IMAGE_STORAGE_PATH)

settings = Settings()

# Create necessary directories
os.makedirs(settings.LOCAL_STORAGE_DIR, exist_ok=True)
os.makedirs(settings.image_storage_dir, exist_ok=True)
os.makedirs(settings.TEMP_DIR, exist_ok=True)

================
File: app/latex_compiler.py
================
# app/latex_compiler.py
import os
import re
import subprocess
import tempfile
from pathlib import Path
import shutil
from typing import Optional, List, Dict, Tuple
import asyncio
from datetime import datetime
import logging
import aiohttp
import aiofiles
from fastapi import HTTPException

from app.storage import StorageProvider
from app.config import settings

logger = logging.getLogger("latex-service")

class LatexCompilationError(Exception):
    """Custom exception for LaTeX compilation errors"""
    def __init__(self, message: str, log_content: str):
        self.message = message
        self.log_content = log_content
        super().__init__(self.message)

class LatexCompiler:
    def __init__(self, storage_provider: StorageProvider, temp_dir: str = "/tmp/latex"):
        self.storage_provider = storage_provider
        self.temp_dir = Path(temp_dir)
        self.texmf_dir = Path("/texmf/tex/latex")  # Local TEXMF directory
        os.makedirs(temp_dir, exist_ok=True)
        os.makedirs(self.texmf_dir, exist_ok=True)

    async def _prepare_class_files(self, work_dir: Path, project_id: str) -> None:
        """Copy required class files to the working directory"""
        logger.debug("Preparing class files")
        try:
            # Get list of class files
            cls_response = await self.storage_provider.list_images(project_id)  # This should list class files too
            class_files = [f for f in cls_response if f.name.endswith('.cls')]

            for cls_file in class_files:
                # Get the class file content
                response = await fetch(f"/api/cls/{cls_file.name}")
                if response.ok:
                    content = await response.text()
                    
                    # Write to both working directory and texmf directory
                    cls_paths = [
                        work_dir / cls_file.name,
                        self.texmf_dir / cls_file.name
                    ]
                    
                    for cls_path in cls_paths:
                        async with aiofiles.open(cls_path, 'w') as f:
                            await f.write(content)
                            logger.debug(f"Wrote class file to {cls_path}")

            # Run texhash to update the database
            process = await asyncio.create_subprocess_exec(
                "texhash",
                str(self.texmf_dir),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()
            logger.debug("Updated TeX database")

        except Exception as e:
            logger.error(f"Error preparing class files: {str(e)}")
            raise

    async def _download_image(self, url: str, target_path: Path) -> None:
        """Download image from URL to target path"""
        try:
            logger.debug(f"Downloading image from {url} to {target_path}")
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status != 200:
                        raise Exception(f"Failed to download image: {url}, status: {response.status}")
                    
                    async with aiofiles.open(target_path, 'wb') as f:
                        await f.write(await response.read())
            logger.debug(f"Successfully downloaded image to {target_path}")
        except Exception as e:
            logger.error(f"Error downloading image {url}: {str(e)}")
            raise

    async def _process_images(self, tex_content: str, work_dir: Path, project_id: str) -> str:
        """Process images in TeX content and return updated content"""
        logger.debug("Processing images in LaTeX content")
        
        # Regular expression to find image inclusions
        image_pattern = r'\\includegraphics(?:\[.*?\])?\{(.*?)\}'
        
        async def process_match(match) -> str:
            image_path = match.group(1)
            logger.debug(f"Processing image reference: {image_path}")
            
            try:
                # If it's a URL or storage path, download the image
                if image_path.startswith(('http://', 'https://', 'storage/', project_id)):
                    # Get the URL for the image
                    if image_path.startswith(('storage/', project_id)):
                        logger.debug(f"Getting URL for storage path: {image_path}")
                        image_url = await self.storage_provider.get_image_url(image_path)
                    else:
                        image_url = image_path
                    
                    # Generate local filename
                    ext = os.path.splitext(image_path)[1] or '.png'
                    local_filename = f"img_{hash(image_path)}{ext}"
                    local_path = work_dir / local_filename
                    
                    # Download the image
                    await self._download_image(image_url, local_path)
                    logger.debug(f"Image downloaded and saved as {local_filename}")
                    
                    # Return the local path for LaTeX
                    return f"\\includegraphics{{{local_filename}}}"
                
                logger.debug(f"Using original image path: {image_path}")
                return match.group(0)
            except Exception as e:
                logger.error(f"Error processing image {image_path}: {str(e)}")
                raise Exception(f"Failed to process image {image_path}: {str(e)}")

        # Process all image references
        positions = []
        for match in re.finditer(image_pattern, tex_content):
            positions.append((match.start(), match.end(), match.group(1)))

        # Process images concurrently
        if positions:
            logger.debug(f"Found {len(positions)} images to process")
            new_strings = await asyncio.gather(
                *(process_match(match) for match in re.finditer(image_pattern, tex_content))
            )
            
            # Replace all matches with processed strings
            result = list(tex_content)
            for (start, end, _), new_string in zip(positions[::-1], new_strings[::-1]):
                result[start:end] = new_string
            
            tex_content = ''.join(result)
            logger.debug("Finished processing all images")

        return tex_content

    async def _parse_latex_log(self, log_path: Path) -> str:
        """Parse LaTeX log file and extract relevant error information"""
        if not log_path.exists():
            return "No log file found"

        try:
            async with aiofiles.open(log_path, 'r', encoding='utf-8', errors='replace') as f:
                content = await f.read()

            # Extract error messages
            error_pattern = r'!(.*?)l\.\d+'
            errors = re.findall(error_pattern, content, re.DOTALL)
            
            # Extract warnings
            warning_pattern = r'Warning:(.*?)$'
            warnings = re.findall(warning_pattern, content, re.MULTILINE)

            parsed_log = "=== LaTeX Compilation Log ===\n"
            
            if errors:
                parsed_log += "\nErrors:\n"
                for error in errors:
                    parsed_log += f"- {error.strip()}\n"
            
            if warnings:
                parsed_log += "\nWarnings:\n"
                for warning in warnings:
                    parsed_log += f"- {warning.strip()}\n"

            return parsed_log
        except Exception as e:
            logger.error(f"Error parsing LaTeX log: {str(e)}")
            return "Error parsing log file"

    async def _run_pdflatex(self, tex_file: Path, work_dir: Path) -> Tuple[bool, str]:
        """Run pdflatex and capture output"""
        try:
            logger.debug(f"Running pdflatex on {tex_file.name}")
            process = await asyncio.create_subprocess_exec(
                "pdflatex",
                "-interaction=nonstopmode",
                "-file-line-error",
                tex_file.name,
                cwd=str(work_dir),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()
            
            # Log the raw output
            logger.debug("pdflatex stdout:")
            logger.debug(stdout.decode('utf-8', errors='replace'))
            if stderr:
                logger.debug("pdflatex stderr:")
                logger.debug(stderr.decode('utf-8', errors='replace'))

            # Parse the log file
            log_file = work_dir / f"{tex_file.stem}.log"
            log_content = await self._parse_latex_log(log_file)
            
            if process.returncode != 0:
                logger.error("pdflatex compilation failed")
                logger.error(log_content)
                return False, log_content
            
            return True, log_content

        except Exception as e:
            error_msg = f"Error running pdflatex: {str(e)}"
            logger.error(error_msg)
            return False, error_msg

    async def compile(self, tex_content: str, project_id: str, timeout: Optional[int] = None) -> Path:
        """Compile LaTeX content with images and return path to PDF"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        work_dir = Path(self.temp_dir) / f"compile_{timestamp}"
        work_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Starting LaTeX compilation for project {project_id}")
        logger.debug(f"Working directory: {work_dir}")

        try:
            # Prepare class files before compilation
            await self._prepare_class_files(work_dir, project_id)

            # Process images in the content
            logger.debug("Processing images in content")
            tex_content = await self._process_images(tex_content, work_dir, project_id)

            # Write TEX content to file
            tex_file = work_dir / "document.tex"
            async with aiofiles.open(tex_file, 'w') as f:
                await f.write(tex_content)
            logger.debug(f"LaTeX content written to {tex_file}")

            # Set TEXMFHOME environment variable
            os.environ['TEXMFHOME'] = str(self.texmf_dir.parent)

            # Run pdflatex with proper environment
            for compilation_pass in range(2):
                logger.debug(f"Starting compilation pass {compilation_pass + 1}/2")
                
                success, log_content = await self._run_pdflatex(tex_file, work_dir)
                if not success:
                    raise LatexCompilationError(
                        "LaTeX compilation failed",
                        log_content
                    )

            pdf_path = work_dir / "document.pdf"
            if not pdf_path.exists():
                raise LatexCompilationError(
                    "PDF was not generated",
                    "No PDF output file found after compilation"
                )

            # Move to final location
            output_path = Path(self.temp_dir) / f"output_{timestamp}.pdf"
            shutil.move(pdf_path, output_path)
            logger.info(f"Compilation successful, PDF saved to {output_path}")
            
            return output_path

        except Exception as e:
            logger.error(f"Compilation error: {str(e)}")
            raise
        finally:
            # Clean up temporary directory
            try:
                shutil.rmtree(work_dir)
                logger.debug(f"Cleaned up working directory: {work_dir}")
            except Exception as e:
                logger.warning(f"Error cleaning up working directory: {str(e)}")
                
    @staticmethod
    async def cleanup_old_files(max_age_hours: int = 24):
        """Clean up old temporary files"""
        try:
            current_time = datetime.now().timestamp()
            temp_dir = Path(settings.TEMP_DIR)
            
            for file in temp_dir.glob("output_*.pdf"):
                if (current_time - file.stat().st_mtime) > (max_age_hours * 3600):
                    file.unlink()
                    logger.debug(f"Cleaned up old file: {file}")
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")

================
File: app/logging_config.py
================
# app/logging_config.py
import logging
import sys
from pathlib import Path

def setup_logging():
    # Create logs directory if it doesn't exist
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            # Console handler with DEBUG level
            logging.StreamHandler(sys.stdout),
            # File handler with ERROR level
            logging.FileHandler(log_dir / "error.log"),
        ]
    )

    # Set uvicorn access logger level
    logging.getLogger("uvicorn.access").setLevel(logging.INFO)
    
    # Create logger for our application
    logger = logging.getLogger("latex-service")
    return logger

================
File: app/main.py
================
# app/main.py
from fastapi import FastAPI, Request, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import traceback
import os
from typing import Union
import sys

from app.config import settings
from app.api.routes import router
from app.storage import LocalStorageProvider, SupabaseStorageProvider
from app.logging_config import setup_logging
from supabase import create_client

# Setup logging
logger = setup_logging()

app = FastAPI(title="LaTeX Compilation Service")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global error handler for all routes"""
    # Log the error with full traceback
    error_msg = f"Unhandled exception in {request.url.path}: {str(exc)}\n{''.join(traceback.format_tb(exc.__traceback__))}"
    logger.error(error_msg)
    
    # Return a structured error response
    return JSONResponse(
        status_code=500,
        content={
            "detail": "Internal server error",
            "type": type(exc).__name__,
            "path": request.url.path,
            "method": request.method,
            "error": str(exc) if settings.ENV == "development" else "Internal server error"
        }
    )

# Exception handler specifically for HTTPException
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handler for HTTP exceptions"""
    logger.warning(f"HTTP exception in {request.url.path}: {exc.detail}")
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "detail": exc.detail,
            "type": "HTTPException",
            "path": request.url.path
        }
    )

# Initialize storage provider with error handling
try:
    if settings.IS_LOCAL:
        logger.info("Initializing local storage provider")
        storage_provider = LocalStorageProvider(settings.LOCAL_STORAGE_DIR)
        # Mount local storage directory for direct file access
        app.mount("/storage", StaticFiles(directory=settings.LOCAL_STORAGE_DIR), name="storage")
    else:
        logger.info("Initializing Supabase storage provider")
        supabase = create_client(
            settings.SUPABASE_URL,
            settings.SUPABASE_KEY
        )
        storage_provider = SupabaseStorageProvider(supabase, settings.PDF_BUCKET_NAME)

    # Add storage provider to app state
    app.state.storage_provider = storage_provider
except Exception as e:
    logger.error(f"Failed to initialize storage provider: {str(e)}\n{''.join(traceback.format_tb(e.__traceback__))}")
    raise

# Include routes
app.include_router(router)

@app.on_event("startup")
async def startup_event():
    """Startup event handler"""
    logger.info("Starting LaTeX Compilation Service")
    # Log configuration
    logger.info(f"Environment: {settings.ENV}")
    logger.info(f"Storage Type: {'Local' if settings.IS_LOCAL else 'Supabase'}")
    logger.info(f"Storage Directory: {settings.LOCAL_STORAGE_DIR}")
    logger.info(f"Max Image Size: {settings.MAX_IMAGE_SIZE / (1024 * 1024)}MB")

    # Ensure required directories exist
    os.makedirs(settings.LOCAL_STORAGE_DIR, exist_ok=True)
    os.makedirs(settings.TEMP_DIR, exist_ok=True)
    logger.info("Required directories created/verified")

@app.on_event("shutdown")
async def shutdown_event():
    """Shutdown event handler"""
    logger.info("Shutting down LaTeX Compilation Service")
    # Add any cleanup code here if needed

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="debug"
    )

================
File: app/storage.py
================
import os
import shutil
from pathlib import Path
import aiofiles
from typing import List, Tuple, Dict
from datetime import datetime
from fastapi import UploadFile
from abc import ABC, abstractmethod
from supabase import Client
import logging

logger = logging.getLogger("latex-service")

class StorageProvider(ABC):
    @abstractmethod
    async def list_images(self, project_id: str) -> List[Dict]:
        """List all files for a project"""
        pass

    @abstractmethod
    async def save_image(self, file: UploadFile, project_id: str) -> Tuple[str, str]:
        """Save an image and return (file_path, url)"""
        pass

    @abstractmethod
    async def save_pdf(self, pdf_path: Path, project_id: str, filename: str) -> Tuple[str, str]:
        """Save PDF and return (file_path, url)"""
        pass

    @abstractmethod
    async def get_image_url(self, file_path: str) -> str:
        """Get URL for an existing image"""
        pass

    @abstractmethod
    async def check_pdf_exists(self, project_id: str, filename: str) -> bool:
        """Check if a PDF exists"""
        pass

    @abstractmethod
    async def get_pdf_url(self, project_id: str, filename: str) -> str:
        """Get URL for an existing PDF"""
        pass

    @abstractmethod
    async def delete_image(self, project_id: str, image_path: str) -> bool:
        """Delete an image"""
        pass

    @abstractmethod
    async def count_images(self, project_id: str) -> int:
        """Count total images for a project"""
        pass

class LocalStorageProvider(StorageProvider):
    def __init__(self, base_dir: str = "storage"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_project_dir(self, project_id: str) -> Path:
        project_dir = self.base_dir / project_id
        project_dir.mkdir(parents=True, exist_ok=True)
        return project_dir

    async def list_images(self, project_id: str, skip: int = 0, limit: int = 50) -> List[Dict]:
        """List all files for a project"""
        try:
            project_dir = self._get_project_dir(project_id)
            
            files = []
            for path in project_dir.rglob("*"):
                if path.is_file():
                    rel_path = path.relative_to(self.base_dir)
                    stats = path.stat()
                    files.append({
                        "name": path.name,
                        "path": str(rel_path),
                        "size": stats.st_size,
                        "modified": datetime.fromtimestamp(stats.st_mtime).isoformat(),
                        "url": f"/storage/{rel_path}"
                    })

            return files[skip:skip + limit]
        except Exception as e:
            logger.error(f"Error listing files for project {project_id}: {str(e)}")
            return []

    async def save_image(self, file: UploadFile, project_id: str, filename: str = None, optimize: bool = False) -> Tuple[str, str]:
        try:
            project_dir = self._get_project_dir(project_id)
            images_dir = project_dir / "images"
            images_dir.mkdir(exist_ok=True)

            if not filename:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                ext = os.path.splitext(file.filename)[1]
                filename = f"img_{timestamp}{ext}"

            file_path = images_dir / filename

            async with aiofiles.open(file_path, 'wb') as f:
                while chunk := await file.read(8192):
                    await f.write(chunk)

            rel_path = file_path.relative_to(self.base_dir)
            url = f"/storage/{rel_path}"

            return str(rel_path), url

        except Exception as e:
            logger.error(f"Error saving image: {str(e)}")
            raise

    async def save_pdf(self, pdf_path: Path, project_id: str, filename: str) -> Tuple[str, str]:
        try:
            project_dir = self._get_project_dir(project_id)
            target_path = project_dir / filename

            shutil.copy2(pdf_path, target_path)

            rel_path = target_path.relative_to(self.base_dir)
            url = f"/storage/{rel_path}"

            return str(rel_path), url

        except Exception as e:
            logger.error(f"Error saving PDF: {str(e)}")
            raise

    async def get_image_url(self, file_path: str) -> str:
        return f"/storage/{file_path}"

    async def check_pdf_exists(self, project_id: str, filename: str) -> bool:
        project_dir = self._get_project_dir(project_id)
        pdf_path = project_dir / filename
        return pdf_path.exists()

    async def get_pdf_url(self, project_id: str, filename: str) -> str:
        return f"/storage/{project_id}/{filename}"

    async def delete_image(self, project_id: str, image_path: str) -> bool:
        try:
            file_path = self.base_dir / image_path
            if file_path.exists():
                file_path.unlink()
                return True
            return False
        except Exception as e:
            logger.error(f"Error deleting image: {str(e)}")
            return False

    async def count_images(self, project_id: str) -> int:
        try:
            project_dir = self._get_project_dir(project_id)
            return len(list(project_dir.rglob("*")))
        except Exception as e:
            logger.error(f"Error counting images: {str(e)}")
            return 0

class SupabaseStorageProvider(StorageProvider):
    def __init__(self, supabase_client: Client, bucket_name: str = "pdfs"):
        self.supabase = supabase_client
        self.bucket_name = bucket_name

    async def save_pdf(self, pdf_path: Path, project_id: str, filename: str) -> Tuple[str, str]:
        bucket_path = f"{project_id}/{filename}"
        
        with pdf_path.open('rb') as pdf_file:
            self.supabase.storage.from_(self.bucket_name).upload(
                bucket_path,
                pdf_file,
                {"upsert": True}
            )
        
        url = self.supabase.storage.from_(self.bucket_name).create_signed_url(
            bucket_path,
            60 * 60  # 1 hour expiry
        )
        
        return bucket_path, url

    async def save_image(self, image: UploadFile, project_id: str) -> Tuple[str, str]:
        # Generate unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        _, ext = os.path.splitext(image.filename)
        filename = f"img_{timestamp}{ext}"
        
        bucket_path = f"{project_id}/images/{filename}"
        
        # Read the file content
        content = await image.read()
        
        # Upload to Supabase
        self.supabase.storage.from_(self.bucket_name).upload(
            bucket_path,
            content,
            {"upsert": True}
        )
        
        url = self.supabase.storage.from_(self.bucket_name).create_signed_url(
            bucket_path,
            60 * 60 * 24 * 7  # 7 days expiry for images
        )
        
        return bucket_path, url

    async def get_image_url(self, file_path: str) -> str:
        return self.supabase.storage.from_(self.bucket_name).create_signed_url(
            file_path,
            60 * 60 * 24 * 7  # 7 days expiry for images
        )

================
File: scripts/debug.sh
================
#!/bin/bash

# Stop and remove existing containers
docker-compose down

# Remove old logs
rm -f logs/*.log

# Rebuild and start the service
docker-compose up --build

# To view logs in real-time
# docker-compose logs -f latex-compiler

================
File: scripts/install_texlive.sh
================
#!/bin/bash
set -e

# Update package list
apt-get update

# Install required packages
apt-get install -y \
    texlive-latex-base \
    texlive-latex-extra \
    texlive-fonts-recommended \
    texlive-science \
    texlive-publishers \
    latexmk \
    curl

# Create local texmf directory structure
mkdir -p /texmf/tex/latex

# Update TeX database
texhash

# Clean up
apt-get clean
rm -rf /var/lib/apt/lists/*

================
File: .env.example
================
# Environment
ENV=development

# Supabase settings (required in production)
SUPABASE_URL=your-supabase-url
SUPABASE_KEY=your-supabase-key
PDF_BUCKET_NAME=pdfs

# Storage settings
LOCAL_STORAGE_DIR=storage
TEMP_DIR=/tmp/latex

# Compilation settings
MAX_COMPILATION_TIME=300

# Image settings
MAX_IMAGE_SIZE=10  # In MB
IMAGE_STORAGE_PATH=storage/assets
OPTIMIZE_IMAGES=false
MAX_IMAGE_DIMENSION=2000  # Max width/height in pixels

# Debugging
LOG_LEVEL=debug

================
File: docker-compose.yml
================
version: '3.8'

services:
  latex-compiler:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./app:/app/app
      - ./storage:/app/storage
      - texmf-data:/texmf
      - latex-cache:/root/.texlive
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - ENV=development
      - TEXMFHOME=/texmf
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  texmf-data:
  latex-cache:

================
File: Dockerfile
================
# Use Ubuntu as base image
FROM ubuntu:22.04

# Prevent tzdata from asking for geographic area
ENV DEBIAN_FRONTEND=noninteractive

# Install Python and basic requirements
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    python3.10-venv \
    && rm -rf /var/lib/apt/lists/*

# Create and set working directory
WORKDIR /app

# Copy installation script and run it
COPY scripts/install_texlive.sh /tmp/
RUN chmod +x /tmp/install_texlive.sh && /tmp/install_texlive.sh

# Copy requirements and install Python packages
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Set up TEXMF directory
ENV TEXMFHOME=/texmf
RUN mkdir -p /texmf/tex/latex

# Copy application code
COPY app/ app/

# Update TeX database
RUN texhash /texmf

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

================
File: README.md
================
# LaTeX Compilation Service

A FastAPI-based service that compiles LaTeX documents with image support and stores the resulting PDFs either locally or in Supabase storage. Perfect for development and production environments.

## Features

- Async LaTeX compilation with timeout protection
- Dual storage support:
  - Local file storage for development
  - Supabase storage for production
- Automatic cleanup of temporary files
- Docker support for easy deployment
- Health check endpoint
- Configurable settings via environment variables
- Project-based PDF organization

## Prerequisites

- Docker and Docker Compose
- For production: Supabase account with storage bucket configured
- TeX Live packages (automatically installed in Docker)

## Project Structure

```
.
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
├── .env.example
├── README.md
├── scripts/
│   └── install_texlive.sh
└── src/
    ├── __init__.py
    ├── main.py
    ├── config.py
    ├── latex_compiler.py
    └── storage.py
```

## Quick Start

1. Clone the repository:
```bash
git clone <repository-url>
cd latex-compiler-service
```

2. Copy and configure environment variables:
```bash
cp .env.example .env
```

3. For local development:
```bash
# Set in .env
ENV=development
LOCAL_STORAGE_DIR=storage
```

4. For production:
```bash
# Set in .env
ENV=production
SUPABASE_URL=your-supabase-url
SUPABASE_KEY=your-supabase-key
```

5. Build and run with Docker Compose:
```bash
docker-compose up --build
```

The service will be available at http://localhost:8000

## Storage Modes

### Local Storage (Development)
- PDFs are stored in the `storage/` directory
- Files are organized by project: `storage/<project_id>/<filename>.pdf`
- Access files via `http://localhost:8000/storage/<project_id>/<filename>.pdf`
- Perfect for development and testing

### Supabase Storage (Production)
- PDFs are stored in your Supabase bucket
- Files are organized by project: `<project_id>/<filename>.pdf`
- Secure signed URLs for file access
- Ideal for production deployment

## API Endpoints

### Health Check
```
GET /health
Response:
{
    "status": "healthy",
    "mode": "local|production",
    "storage": "local|supabase"
}
```

### Upload Image
```
POST /upload-image/{project_id}
Content-Type: multipart/form-data

file: <image_file>

Response:
{
    "status": "success",
    "file_path": "path/to/image",
    "url": "access_url"
}
```

### Compile LaTeX
```
POST /compile
Content-Type: application/json

{
    "tex_content": "\\documentclass{article}...",
    "project_id": "project-123",
    "output_filename": "document.pdf"
}

Response:
{
    "status": "success",
    "file_path": "path/to/file",
    "url": "access_url",
    "storage_type": "local|supabase"
}
```

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| ENV | Environment (development/production) | development |
| SUPABASE_URL | Supabase project URL | required in production |
| SUPABASE_KEY | Supabase service role key | required in production |
| PDF_BUCKET_NAME | Supabase storage bucket name | pdfs |
| LOCAL_STORAGE_DIR | Local storage directory | storage |
| MAX_COMPILATION_TIME | Max compilation time in seconds | 300 |
| TEMP_DIR | Directory for temporary files | /tmp/latex |
| IMAGE_STORAGE_DIR | Local image storage directory | storage/images |
| MAX_IMAGE_SIZE | Maximum image size in MB | 10 |


## Image Support

The service supports handling images in LaTeX documents through two main workflows:

1. **Upload & Reference**: Upload images first, then reference them in LaTeX
2. **Direct URL**: Use direct URLs in LaTeX (automatically downloaded during compilation)

### Image Storage

- **Local Mode**: Images stored in `storage/<project_id>/images/`
- **Production Mode**: Images stored in Supabase bucket under `<project_id>/images/`
- Automatic organization by project
- Unique filename generation to prevent conflicts

## Image Guidelines

- Supported formats: PNG, JPEG, PDF
- Maximum file size: 10MB (configurable)
- Recommended formats for best results:
  - Photos: JPEG
  - Diagrams/Charts: PNG
  - Vector Graphics: PDF

## Next.js Integration

1. Create an API route in your Next.js project:

```typescript
// src/app/api/compile-latex/route.ts
import { NextResponse } from 'next/server'

const LATEX_SERVICE_URL = process.env.LATEX_SERVICE_URL || 'http://localhost:8000'

export async function POST(request: Request) {
  try {
    const body = await request.json()
    const response = await fetch(`${LATEX_SERVICE_URL}/compile`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(body)
    })
    
    const result = await response.json()
    return NextResponse.json(result)
  } catch (error) {
    return NextResponse.json(
      { error: error instanceof Error ? error.message : 'Unknown error occurred' },
      { status: 500 }
    )
  }
}
```

2. Use in your frontend:

```typescript
const response = await fetch('/api/compile-latex', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    texContent: '\\documentclass{article}...',
    projectId: 'project-123',
    filename: 'document.pdf'
  })
})

const result = await response.json()
console.log(result.url) // Access the compiled PDF
```

## Development

### Adding New LaTeX Packages

1. Update `scripts/install_texlive.sh`:
```bash
apt-get install -y \
    texlive-latex-base \
    texlive-latex-extra \
    texlive-fonts-recommended \
    texlive-science \
    your-new-package
```

2. Rebuild the Docker image:
```bash
docker-compose build --no-cache
```

### Local Development Without Docker

1. Install system dependencies:
```bash
sudo apt-get update
sudo apt-get install -y texlive-latex-base texlive-latex-extra
```

2. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3. Run the service:
```bash
python -m uvicorn app.main:app --reload
```

## Deployment

### Using Docker

1. Build the image:
```bash
docker build -t latex-compiler .
```

2. Run in production:
```bash
docker run -d \
  -p 8000:8000 \
  --env-file .env \
  --name latex-compiler \
  latex-compiler
```

### Using Docker Compose

1. Update environment variables in `.env`
2. Deploy:
```bash
docker-compose -f docker-compose.prod.yml up -d
```

## Monitoring

- Health check endpoint: `GET /health`
- Docker health check is configured
- Logs available via `docker logs latex-compiler`

## Troubleshooting

### Common Image Issues

1. **Image Not Found**
   - Check if the image path is correct
   - Verify image was uploaded successfully
   - Check storage permissions

2. **Compilation Errors with Images**
   - Ensure `\usepackage{graphicx}` is included
   - Check image file format compatibility
   - Verify image file is not corrupted

3. **Image Quality Issues**
   - Use appropriate format for image type
   - Check original image resolution
   - Consider using PDF format for vector graphics

### Debug Mode

Set `LOG_LEVEL=debug` in `.env` for detailed logging:
```bash
LOG_LEVEL=debug
```

## Debugging
- Check general application logs
```bash
docker-compose logs -f latex-compiler
```

- Check error logs specifically:
```bash
tail -f logs/error.log
```

- Filter logs by severity:
```bash
docker-compose logs -f latex-compiler | grep "ERROR"
```

- Search for specific operations:
```bash
docker-compose logs -f latex-compiler | grep "Compiling"
```

## Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

MIT License - feel free to use this project for your own purposes.

================
File: requirements.txt
================
fastapi==0.104.1
uvicorn==0.24.0
python-dotenv==1.0.0
pydantic==2.4.2
supabase==2.0.3
python-multipart==0.0.6
pillow===11.0.0
aiohttp===3.10.10
aiofiles===24.1.0

================
File: structure.txt
================
latex-service/
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
├── .env.example
├── README.md
├── scripts/
│   └── install_texlive.sh
└── app/
    ├── __init__.py
    ├── main.py
    ├── config.py
    ├── storage.py
    ├── latex_compiler.py
    └── api/
        ├── __init__.py
        └── routes.py
